{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature = 0.5) # temperature is a measure of randomness\n",
    "#print(llm.predict('How old was donald trump in 1987?')) (have to register and uses openAPI credits to use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (AIMessage,HumanMessage,SystemMessage)\n",
    "\n",
    "chat = ChatOpenAI(temperature = 0.5)\n",
    "#print(chat.predict_messages([HumanMessage(content=\"whats 9+10 (hint, the answer I want is referencing a meme)\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21 (the answer from the \"What\\'s 9+10?\" meme)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"whats 9+10 (hint, the answer I want is referencing a meme)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI language model, I do not have personal preferences or opinions. However, based on online reviews and ratings, some of the best gyms in Soho for athletes are:\\n\\n1. Equinox Soho: This gym offers a range of equipment and classes for athletes, including strength training, cardio, and yoga. It also has a spacious and well-maintained facility.\\n\\n2. Barry's Bootcamp: This gym is known for its high-intensity interval training (HIIT) classes that are designed to challenge athletes and improve their fitness levels.\\n\\n3. SoulCycle Soho: This gym offers indoor cycling classes that are popular among athletes looking to improve their endurance and cardiovascular health.\\n\\n4. Fhitting Room: This gym offers group classes that combine strength training and HIIT workouts, making it a great option for athletes looking for a challenging and varied workout.\\n\\n5. Rumble Boxing: This gym offers boxing-inspired workouts that are great for athletes looking to improve their strength, agility, and coordination.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "genericTemplate = PromptTemplate.from_template(\"What is the best {amenity} in {location} for {reason}\")\n",
    "gymPrompt = template.format(amenity = \"gym\", location = \"Soho\", reason = \"athletes\")\n",
    "chat.predict(gymPrompt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playing with Chains**\\\n",
    "These pass your input first to a PromptTemplate which is then passed to an LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nThe best gym in Soho for athletes is Equinox Soho. It offers a wide range of equipment and classes for athletes, as well as a wide variety of amenities. It also has a team of expert trainers to help you reach your fitness goals.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm = llm, prompt = prompt)\n",
    "#chain.run({'amenity':\"gym\", 'location':\"Soho\", 'reason':\"athletes\"})\n",
    "chain.run(amenity= \"gym\", location= \"Soho\", reason= \"athletes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roles**\n",
    "- Notice how in the following example, we use more specific roles (chat, system & human), rather than just the generic template we used previously (these are specifc to openAI)\n",
    "- The 'System' role can be used to declare messages that determine the behaviour/ personality of the 'Assistant'. For example, your typical \"You are XGPT\" type of prompts. \n",
    "- The 'User' sends requests for the 'Assistant' to respond to.\n",
    "\n",
    "**Why Chat Models?**\\\n",
    "The APIs are stateless, so the inputs you send are all the context the LLM will have. Chats act as a memory of sorts, collating all neccessary context you'd want based on a series of messages/prompts, e.g.:\n",
    "\n",
    "~~~\n",
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "~~~\n",
    "\n",
    "**And What about Chains?**\\\n",
    "Chains provide a way to link the various primitives surrounding LLMs (e.g. prompts, models and other chains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey friend, what's up?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "GPT = ChatOpenAI(temperature=1)\n",
    "\n",
    "system_template = \"You are a linguistic guru that translates {input} to {output}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_template = \"{initial_text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt]) #input variables could be explicitly declared here but don't need to be, unlike for sequential chains\n",
    "chain = LLMChain(llm=GPT, \n",
    "                 prompt = chat_prompt,\n",
    "                 output_key=\"initial_translation\") # Here we've 'chained' togeather a model and a prompt, note we didn't need to declare input variables here\n",
    "chain.run(input = \"Spanish\",output = \"English\", initial_text = \"Que pasa amigo?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chaining the Chains (or should i say combining the links to form a longer chain, either or, hopefully you comprende)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Spanish',\n",
       " 'output': 'English',\n",
       " 'initial_text': 'Que pasa amigo',\n",
       " 'text': '\"Quid novi, amice?\" (in Latin)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "human_message_prompt_2 = HumanMessagePromptTemplate.from_template(\"Translate the secondary language {initial_translation} into a random latin based language\")\n",
    "chat_prompt_2 = ChatPromptTemplate.from_messages([human_message_prompt_2])\n",
    "chain_2 = LLMChain(llm = GPT, \n",
    "                   prompt = chat_prompt_2)\n",
    "\n",
    "combined_chain = SequentialChain(chains = [chain, chain_2], \n",
    "                                 input_variables=[\"input\", \"output\", \"initial_text\"], # note the need for the declaration of input variables here\n",
    "                                 verbose =True,)\n",
    "combined_chain({\"input\": \"Spanish\", \"output\": \"English\", \"initial_text\": \"Que pasa amigo\"}) # can do this instead of chain.run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agents**\\\n",
    "Agents provide autonomy in the way a workflow is undertaken. Unlike chains that use predefined steps, these use LLMs to determine their next prompt and then take action.\\\n",
    "Each agent needs a: tool (e.g. search engine, database lookup, etc) in order to produce a final output, LLM and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the temperature first and then raise it to a power afterwards\n",
      "Action: Search\n",
      "Action Input: Peak temperature in Wandsworth London yesterday\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.timeanddate.com/weather/@2634812/historic\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m It looks like the highest temperature yesterday was 19°C\n",
      "Action: Calculator\n",
      "Action Input: 19^0.035\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1085527815934937\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1.1085527815934937\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1085527815934937'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=1) #Use GPT as the LLM for the agent\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm = llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True) # Note, ZERO_SHOT_REACT_DESCRIPTION, is the default agent type anyway\n",
    "agent.run(\"What was the peak temperature in Wandsworth London yesterday in degrees Celcius? What is that number raised to the .035 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doing the same as above but with chat models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: Can you produce a summary of the key players in the open source large language model ecosystem?\n",
      "Thought: This is an informative question, and we need to use a search engine to find the information.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"key players in the open source large language model ecosystem\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOpen Source Large Language Models (LLM) – Top 10 Most Powerful To Consider In 2023\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThis article seems promising and relevant to the question, I will read through it and take note of the key players mentioned.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"https://www.analyticsinsight.net/open-source-large-language-models-llm-top-10-most-powerful-to-consider-in-2023/\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe top leading language models for NLP in 2023 are trained to understand data in text or audio format. NLP or Natural Language Processing ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLet me continue reading and see if the article mentions specific key players in the open source LLM ecosystem.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"open source large language model key players\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOpen Source Large Language Models (LLM) – Top 10 Most Powerful To Consider In 2023\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThis is the same article as before, let me search for a different source that specifically mentions key players.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"key players in open source language models\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTop Open Source Large Language Models · 1. GPT-Neo, GPT-J, and GPT-NeoX · 2. XLNet · 3. Roberta · 4. DeBERTa · 5. XLM-RoBERTa · 6. DistilBERT.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe article lists several key players in the open source LLM ecosystem, including GPT-Neo, XLNet, Roberta, DeBERTa, XLM-RoBERTa, and DistilBERT. These models are among the most powerful and widely used in the field of natural language processing. \n",
      "Final Answer: Some of the key players in the open source large language model ecosystem include GPT-Neo, XLNet, Roberta, DeBERTa, XLM-RoBERTa, and DistilBERT.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Some of the key players in the open source large language model ecosystem include GPT-Neo, XLNet, Roberta, DeBERTa, XLM-RoBERTa, and DistilBERT.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "#chat model\n",
    "chat = ChatOpenAI(temperature = 1)\n",
    "\n",
    "#tools\n",
    "llm = OpenAI(temperature=1)\n",
    "tools = load_tools ([\"serpapi\", \"llm-math\"], llm = llm)\n",
    "\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose = True)\n",
    "\n",
    "agent.run(\"Produce a summary of the key players in the open source large language model ecosystem\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory**\\\n",
    "The following example implements a simple buffer memory store, via the 'conversation' primitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What's up doc?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! I'm doing well. How about you?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature = 1)\n",
    "conversation = ConversationChain(llm=llm, verbose = True)\n",
    "conversation.run(\"What's up doc?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What's up doc?\n",
      "AI:  Hi there! I'm doing well. How about you?\n",
      "Human: I am gucci doc, whats on the agenda today?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Well, I was about to review your daily calendar and take a look at what activities you have planned for today. Is there anything in particular you'd like me to focus on or help you with?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run('I am gucci doc, whats on the agenda today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
